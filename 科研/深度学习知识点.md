# 深度学习知识点

- ## tensor格式的意义

  ​	 tensor 格式的数据 B(Batch) C(Chanel) H(Height) W(Width)

- ​	一些代码的作用

  ```python
   # 它会将模型中所有可训练的参数的梯度清零。在训练神经网络时，通常需要在每次迭代之前调用这个函数。因为如果不清零梯度，那么优化器在更新权重时会累加之前的梯度
  optimizer.zero_grad() 
  ```

  ```python
  # 计算损失函数对模型参数的梯度。这个梯度将用于更新模型的参数，以便在下一次训练迭代中减小损失。
  loss.backward()
  ```

  ```python
  #根据优化算法（如梯度下降）更新模型的参数，使其朝着更优的方向调整。这样，在下一次训练迭代中，模型就可以根据新的参数进行更准确的预测。
  optimizer.step()
  ```

## MINIST手写数据集识别

```python
import torch
from torchvision import datasets
from torchvision import transforms
from torch.utils.data import DataLoader
import torchvision
from torch import nn
import torch.nn.functional as F
import torch.optim as optim

batch_size =64
seed =1
learning_rate = 0.01
momentum=0.5
torch.manual_seed(seed)
trans =transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.1307,),(0.3801,))])

train_dataset = torchvision.datasets.MNIST(root='files',train=True,download=True,transform=trans)
test_dataset = torchvision.datasets.MNIST(root='files',train=False,download=True,transform=trans)

train_dataloader = DataLoader(dataset=train_dataset,shuffle=True,batch_size=batch_size)
test_dataloader = DataLoader(dataset=test_dataset,shuffle=False,batch_size=batch_size)

class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)
        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)
        self.conv2_drop = nn.Dropout2d()
        self.fc1 = nn.Linear(320, 50)
        self.fc2 = nn.Linear(50, 10)

    def forward(self, x):
        x = F.relu(F.max_pool2d(self.conv1(x), 2))
        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))
        x = x.view(-1, 320)
        x = F.relu(self.fc1(x))
        x = F.dropout(x, training=self.training)
        x = self.fc2(x)
        return x



model = Net()

loss_fn = nn.CrossEntropyLoss()
optimzer = optim.SGD(model.parameters(),lr=learning_rate,momentum=momentum)


def train(epoch):
    loss =0.0
    for batch_id,data in enumerate(train_dataloader):
        inputs,target = data
        optimzer.zero_grad()
        output = model(inputs)
        lossvalue = loss_fn(output,target)
        lossvalue.backward()
        optimzer.step()

        loss+=lossvalue.item()

        if batch_id%300==0:
            print(f"epoch={epoch},batch={batch_id},loss={loss/300}")
            loss=0.0

def test():
    correct = 0
    total = 0
    for data in test_dataloader:
        images, labels = data
        outputs = model(images)
        # 这个torch.max函数可以返回最大值和最大值的下标,那个predict取的是最大值下标,只需要拿它和标签对比即可
        _, predict = torch.max(outputs.data, dim=1)
        total += labels.size(0)  # 总共有多少个标签样本,Nx1
        correct += (predict == labels).sum().item()  # 将我们预测的最有可能的下标与真实标签对比，最后将这个标量取出来
    print('Accuracy on test set: %d %%' % (100 * correct / total))
    if (100 * correct / total >= 97):
        torch.save(model, 'files/model.pth')

if __name__ == '__main__':
    for epoch in range(1,50):
        train(epoch)
        test()
```

